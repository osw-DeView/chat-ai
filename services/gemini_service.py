# services/gemini_service.py

import google.generativeai as genai
from core.config import GEMINI_API_KEY, TAIL_QUESTION_MODEL, EVALUATION_MODEL
from models.interview_models import Message, StructuredEvaluationReport, TurnEvaluation
from typing import List, Dict, Any
import json
import re
import time
from markdown_it import MarkdownIt

# --- ì´ˆê¸° ì„¤ì • ---
genai.configure(api_key=GEMINI_API_KEY)
generation_config = {"temperature": 0.7}
tail_question_model = genai.GenerativeModel(model_name=TAIL_QUESTION_MODEL, generation_config=generation_config)
evaluation_model = genai.GenerativeModel(model_name=EVALUATION_MODEL, generation_config=generation_config)
md_parser = MarkdownIt()

# --- ë¹„ë™ê¸° ì„±ëŠ¥ ì¸¡ì • í—¬í¼ í•¨ìˆ˜ ---
async def _generate_content_with_performance_metrics(model, prompt: str) -> (str, Dict[str, Any]):
    """
    ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œì„ í†µí•´ ì‘ë‹µì„ ìƒì„±í•˜ê³ , TTFTì™€ TPSì™€ ê°™ì€ ì„±ëŠ¥ ì§€í‘œë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.
    """
    start_time = time.time()
    
    stream = await model.generate_content_async(prompt, stream=True)
    
    first_chunk_time = None
    full_response_text = ""
    
    async for chunk in stream:
        if chunk.text:
            if first_chunk_time is None:
                first_chunk_time = time.time()
            full_response_text += chunk.text

    end_time = time.time()

    try:
        token_count_result = await model.count_tokens_async(full_response_text)
        total_tokens = token_count_result.total_tokens
    except Exception:
        total_tokens = len(full_response_text) // 2 

    ttft = (first_chunk_time - start_time) * 1000 if first_chunk_time else 0
    total_time = end_time - start_time
    
    pure_generation_time = end_time - first_chunk_time if first_chunk_time else total_time
    if pure_generation_time <= 0:
        pure_generation_time = total_time
        
    tps = total_tokens / pure_generation_time if pure_generation_time > 0 else 0

    performance = {
        "time_to_first_token_ms": round(ttft, 2),
        "total_generation_time_s": round(total_time, 2),
        "tokens_per_second": round(tps, 2),
        "total_tokens": total_tokens
    }
    
    return full_response_text, performance

# --- ë§ˆí¬ë‹¤ìš´ ì œê±° í—¬í¼ í•¨ìˆ˜ ---
def _strip_markdown(text: str) -> str:
    """
    Markdown í…ìŠ¤íŠ¸ë¥¼ ë Œë”ë§í•œ í›„ HTML íƒœê·¸ë¥¼ ì œê±°í•˜ì—¬ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    html = md_parser.render(text)
    plain_text = re.sub('<[^<]+?>', '', html)
    return re.sub(r'\n{2,}', '\n', plain_text).strip()

# --- ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜ ---
async def generate_tail_question(conversation: List[Message]) -> Dict[str, Any]:
    """
    ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ê¼¬ë¦¬ ì§ˆë¬¸ì„ ë¹„ë™ê¸°ë¡œ ìƒì„±í•˜ê³  ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
    """
    prompt = """
    ë‹¹ì‹ ì€ IT ê¸°ì—…ì˜ ìˆ™ë ¨ëœ ê¸°ìˆ  ë©´ì ‘ê´€ì…ë‹ˆë‹¤.
    ì•„ë˜ ëŒ€í™”ëŠ” ì§€ì›ìì™€ì˜ CS ê¸°ìˆ  ë©´ì ‘ ë‚´ìš©ì…ë‹ˆë‹¤.
    ì§€ì›ìì˜ ë§ˆì§€ë§‰ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ, ê·¸ì˜ ì§€ì‹ì„ ë” ê¹Šê²Œ íŒŒê³ ë“¤ ìˆ˜ ìˆëŠ” ë‚ ì¹´ë¡œìš´ ê¼¬ë¦¬ ì§ˆë¬¸ì„ 'í•œê¸€ë¡œ' ê·¸ë¦¬ê³  'í•˜ë‚˜ë§Œ' ìƒì„±í•´ ì£¼ì„¸ìš”.
    ì§ˆë¬¸ì€ ê°„ê²°í•˜ê³  ëª…í™•í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ë¶€ê°€ì ì¸ ì„¤ëª… ì—†ì´ ì§ˆë¬¸ ë‚´ìš©ë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”.

    [ëŒ€í™” ë‚´ìš©]
    {chat_history}
    """.format(chat_history="\n".join([f"{msg.role}: {msg.content}" for msg in conversation]))

    response_text, performance = await _generate_content_with_performance_metrics(tail_question_model, prompt)
    cleaned_response = _strip_markdown(response_text)
    return {"response": cleaned_response, "performance": performance}

# --- í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„± í—¬í¼ í•¨ìˆ˜ ---
def _format_for_evaluation(conversation: List[Message]) -> str:
    """
    ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ Gemini ëª¨ë¸ì— ì „ë‹¬í•  ìµœì¢… í‰ê°€ ì§€ì‹œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    interview_record = ""
    dialogue = [msg for msg in conversation if msg.role in ["assistant", "user"]]
    turn = 1
    for i in range(0, len(dialogue), 2):
        if i + 1 < len(dialogue):
            question, answer = dialogue[i].content, dialogue[i+1].content
            interview_record += f"### í„´ {turn}\n**[ì§ˆë¬¸]**\n{question}\n\n**[ë‹µë³€]**\n{answer}\n---\n\n"
            turn += 1

    return f"""
    ë‹¹ì‹ ì€ ì§€ì›ìì˜ ê¸°ìˆ ì  ê¹Šì´ë¥¼ í‰ê°€í•˜ëŠ” IT ì „ë¬¸ ë©´ì ‘ê´€ì…ë‹ˆë‹¤.
    ë‹¤ìŒì€ í•œ ì§€ì›ìì™€ì˜ ì „ì²´ CS ê¸°ìˆ  ë©´ì ‘ ëŒ€í™”ë¡ì…ë‹ˆë‹¤.
    ì´ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§€ì›ìì˜ CS ì§€ì‹ì„ í‰ê°€í•˜ê³ , ì•„ë˜ [ì¶œë ¥ í˜•ì‹]ì„ 'ë°˜ë“œì‹œ' ì¤€ìˆ˜í•˜ì—¬ 'í•œê¸€ë¡œ' ì‘ë‹µí•´ ì£¼ì„¸ìš”.

    # [ë©´ì ‘ ê¸°ë¡]
    {interview_record}
    ---
    # [ì§€ì‹œì‚¬í•­]
    ìœ„ [ë©´ì ‘ ê¸°ë¡]ì„ ë°”íƒ•ìœ¼ë¡œ, ì§€ì›ìì— ëŒ€í•œ ìµœì¢… ì¢…í•© í‰ê°€ë¥¼ ì•„ë˜ [ì¶œë ¥ í˜•ì‹]ì— ë§ì¶° ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ìƒì„±í•˜ì‹­ì‹œì˜¤.

    # [ì¶œë ¥ í˜•ì‹]
    # ìµœì¢… ì¢…í•© í‰ê°€
    **- ì¢…í•© ì ìˆ˜:** (1-100 ì‚¬ì´ì˜ ì •ìˆ˜ ì ìˆ˜)
    **- ì¢…í•© í”¼ë“œë°±:** (ì¢…í•©ì ì¸ ê°•ì ê³¼ ì•½ì ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½)
    **- ê°œì„  í‚¤ì›Œë“œ:**
        - (í•µì‹¬ ê°œì„  í‚¤ì›Œë“œ 1)
        - (í•µì‹¬ ê°œì„  í‚¤ì›Œë“œ 2)
    ---
    ## ì§ˆë¬¸ë³„ ìƒì„¸ í‰ê°€
    ### í„´ 1: (ì²« ë²ˆì§¸ ì§ˆë¬¸ ë‚´ìš© ìš”ì•½)
    **- ì ìˆ˜:** (1-100 ì‚¬ì´ì˜ ì •ìˆ˜ ì ìˆ˜)
    **- í”¼ë“œë°±:** (ì²« ë²ˆì§¸ ë‹µë³€ì— ëŒ€í•œ êµ¬ì²´ì ì¸ í”¼ë“œë°±)

    ### í„´ 2: (ë‘ ë²ˆì§¸ ì§ˆë¬¸ ë‚´ìš© ìš”ì•½)
    **- ì ìˆ˜:** (1-100 ì‚¬ì´ì˜ ì •ìˆ˜ ì ìˆ˜)
    **- í”¼ë“œë°±:** (ë‘ ë²ˆì§¸ ë‹µë³€ì— ëŒ€í•œ êµ¬ì²´ì ì¸ í”¼ë“œë°±)

    ### í„´ 3: (ì„¸ ë²ˆì§¸ ì§ˆë¬¸ ë‚´ìš© ìš”ì•½)
    **- ì ìˆ˜:** (1-100 ì‚¬ì´ì˜ ì •ìˆ˜ ì ìˆ˜)
    **- í”¼ë“œë°±:** (ì„¸ ë²ˆì§¸ ë‹µë³€ì— ëŒ€í•œ êµ¬ì²´ì ì¸ í”¼ë“œë°±)
    """

# --- í‰ê°€ ê²°ê³¼ íŒŒì‹± í—¬í¼ í•¨ìˆ˜ ---
def _parse_structured_evaluation_report(report_text: str) -> StructuredEvaluationReport:
    """
    Geminiê°€ ìƒì„±í•œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ í‰ê°€ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ StructuredEvaluationReport ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    try:
        overall_text, turns_text = re.split(r'\n## ì§ˆë¬¸ë³„ ìƒì„¸ í‰ê°€\n', report_text, 1)

        score_match = re.search(r"\*\*- ì¢…í•© ì ìˆ˜:\*\*\s*(\d+)", overall_text)
        feedback_match = re.search(r"\*\*- ì¢…í•© í”¼ë“œë°±:\*\*\s*(.*?)(?=\n\s*\*\*-|\Z)", overall_text, re.DOTALL)
        keywords_match = re.search(r"\*\*- ê°œì„  í‚¤ì›Œë“œ:\*\*(.*?)(?=\n\n---|\Z)", overall_text, re.DOTALL)
        
        overall_score = int(score_match.group(1)) if score_match else 0
        overall_feedback = feedback_match.group(1).strip() if feedback_match else "ì¢…í•© í”¼ë“œë°±ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        keywords_text = keywords_match.group(1) if keywords_match else ""
        improvement_keywords = [_strip_markdown(line.strip('- ').strip()) for line in keywords_text.strip().split('\n') if line.strip()]

        turn_evaluations = []
        turn_sections = re.split(r"### í„´ \d+:", turns_text)[1:]
        for i, section in enumerate(turn_sections, 1):
            question_match = re.search(r"(.*?)\n\s*\*\*- ì ìˆ˜:", section, re.DOTALL)
            turn_score_match = re.search(r"\*\*- ì ìˆ˜:\*\*\s*(\d+)", section)
            turn_feedback_match = re.search(r"\*\*- í”¼ë“œë°±:\*\*\s*(.*)", section, re.DOTALL)

            turn_evaluations.append(TurnEvaluation(
                turn=i,
                question=_strip_markdown(question_match.group(1).strip() if question_match else "ì§ˆë¬¸ ì—†ìŒ"),
                score=int(turn_score_match.group(1)) if turn_score_match else 0,
                feedback=turn_feedback_match.group(1).strip() if turn_feedback_match else "í”¼ë“œë°± ì—†ìŒ"
            ))

        return StructuredEvaluationReport(
            overall_score=overall_score,
            overall_feedback=overall_feedback,
            improvement_keywords=improvement_keywords,
            turn_evaluations=turn_evaluations
        )
    except Exception as e:
        print(f"í‰ê°€ ë³´ê³ ì„œ íŒŒì‹± ì‹¤íŒ¨: {e}\nì›ë³¸ í…ìŠ¤íŠ¸:\n{report_text}")
        return StructuredEvaluationReport(
            overall_score=0,
            overall_feedback=f"ë¦¬í¬íŠ¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì´ ìƒì„±í•œ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”:\n\n{report_text}",
            improvement_keywords=[],
            turn_evaluations=[]
        )

# --- ì¢…í•© í‰ê°€ í•¨ìˆ˜ ---
async def evaluate_conversation(conversation: List[Message]) -> Dict[str, Any]:
    """
    ì „ì²´ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë©´ì ‘ì„ ë¹„ë™ê¸°ë¡œ í‰ê°€í•˜ê³ , ì„±ëŠ¥ì„ ì¸¡ì •í•œ ë’¤ êµ¬ì¡°í™”ëœ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    prompt = _format_for_evaluation(conversation)
    markdown_response, performance = await _generate_content_with_performance_metrics(evaluation_model, prompt)
    structured_report = _parse_structured_evaluation_report(markdown_response.strip())
    
    # --- ğŸ‘‡ ì‚¬ìš©ìë‹˜ê»˜ì„œ ì œì•ˆí•˜ì‹  ìˆ˜ì • ì‚¬í•­ ---
    # performanceëŠ” ê³„ì‚°ë˜ì§€ë§Œ, ìµœì¢… ë°˜í™˜ ë”•ì…”ë„ˆë¦¬ì—ì„œëŠ” ì œì™¸ë©ë‹ˆë‹¤.
    return {
        "evaluation_report": structured_report,
        # "performance": performance 
    }